game: python_mfg_dynamic_routing

GameType.chance_mode = ChanceMode.EXPLICIT_STOCHASTIC
GameType.dynamics = Dynamics.MEAN_FIELD
GameType.information = Information.PERFECT_INFORMATION
GameType.long_name = "Python Mean Field Routing Game"
GameType.max_num_players = 1
GameType.min_num_players = 1
GameType.parameter_specification = ["players"]
GameType.provides_information_state_string = True
GameType.provides_information_state_tensor = True
GameType.provides_observation_string = True
GameType.provides_observation_tensor = True
GameType.provides_factored_observation_string = True
GameType.reward_model = RewardModel.TERMINAL
GameType.short_name = "python_mfg_dynamic_routing"
GameType.utility = Utility.GENERAL_SUM

NumDistinctActions() = 5
PolicyTensorShape() = [5]
MaxChanceOutcomes() = 2
GetParameters() = {players=-1}
NumPlayers() = 1
MinUtility() = -3.0
MaxUtility() = 0.0
UtilitySum() = 0.0
InformationStateTensorShape() = [3]
InformationStateTensorLayout() = TensorLayout.CHW
InformationStateTensorSize() = 3
ObservationTensorShape() = [3]
ObservationTensorLayout() = TensorLayout.CHW
ObservationTensorSize() = 3
MaxGameLength() = 2
ToString() = "python_mfg_dynamic_routing(players=-1)"

# State 0
# Before initial chance node
IsTerminal() = False
History() = []
HistoryString() = ""
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = "Before initial chance node"
InformationStateTensor(0).observation = [-1.0, 0.0, 0.0]
ObservationString(0) = "Before initial chance node"
PublicObservationString() = "Before initial chance node"
PrivateObservationString(0) = "Before initial chance node"
ObservationTensor(0) = [-1.0, 0.0, 0.0]
ChanceOutcomes() = [(0, 1.0)]
LegalActions() = [0]
StringLegalActions() = ["Vehicle is assigned to population 0."]

# Apply action "Vehicle is assigned to population 0."
action: 0

# State 1
# Location=bef_O->O, movement=True, t=0, destination='D->aft_D'
IsTerminal() = False
History() = [0]
HistoryString() = "0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "Location=bef_O->O, movement=True, t=0, destination='D->aft_D'"
InformationStateTensor(0).observation: ◉◯◯
ObservationString(0) = "Location=bef_O->O, movement=True, t=0, destination='D->aft_D'"
PublicObservationString() = "Location=bef_O->O, movement=True, t=0, destination='D->aft_D'"
PrivateObservationString(0) = "Location=bef_O->O, movement=True, t=0, destination='D->aft_D'"
ObservationTensor(0): ◉◯◯
Rewards() = [0.0]
Returns() = [0]
LegalActions() = [2]
StringLegalActions() = ["Vehicle 0 would like to move to O->A."]

# Apply action "Vehicle 0 would like to move to O->A."
action: 2

# State 2
# Location=O->A, movement=True, t=1_mean_field, destination='D->aft_D'
IsTerminal() = False
History() = [0, 2]
HistoryString() = "0, 2"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.MEAN_FIELD
InformationStateString(0) = "Location=O->A, movement=True, t=1_mean_field, destination='D->aft_D'"
InformationStateTensor(0).observation = [1.0, 2.0, 0.0]
ObservationString(0) = "Location=O->A, movement=True, t=1_mean_field, destination='D->aft_D'"
PublicObservationString() = "Location=O->A, movement=True, t=1_mean_field, destination='D->aft_D'"
PrivateObservationString(0) = "Location=O->A, movement=True, t=1_mean_field, destination='D->aft_D'"
ObservationTensor(0) = [1.0, 2.0, 0.0]
Rewards() = [0.0]
Returns() = [0]
DistributionSupport() = ["Location=O->A, movement=True, t=1_mean_field, destination='D->aft_D'", "Location=O->A, movement=False, t=1_mean_field, destination='D->aft_D'"]

# Set mean field distribution to be uniform
action: update_distribution

# State 3
# Location=O->A, movement=True, t=1_chance, destination='D->aft_D'
IsTerminal() = False
History() = [0, 2]
HistoryString() = "0, 2"
IsChanceNode() = True
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.CHANCE
InformationStateString(0) = "Location=O->A, movement=True, t=1_chance, destination='D->aft_D'"
InformationStateTensor(0).observation = [1.0, 2.0, 0.0]
ObservationString(0) = "Location=O->A, movement=True, t=1_chance, destination='D->aft_D'"
PublicObservationString() = "Location=O->A, movement=True, t=1_chance, destination='D->aft_D'"
PrivateObservationString(0) = "Location=O->A, movement=True, t=1_chance, destination='D->aft_D'"
ObservationTensor(0) = [1.0, 2.0, 0.0]
ChanceOutcomes() = [(1, 0.009900990099009901), (0, 0.9900990099009901)]
LegalActions() = [1, 0]
StringLegalActions() = ["Change node; the vehicle movement is True.", "Change node; the vehicle movement is False."]

# Apply action "Change node; the vehicle movement is False."
action: 0

# State 4
# Location=O->A, movement=False, t=1, destination='D->aft_D'
IsTerminal() = False
History() = [0, 2, 0]
HistoryString() = "0, 2, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.DEFAULT_PLAYER_ID
InformationStateString(0) = "Location=O->A, movement=False, t=1, destination='D->aft_D'"
InformationStateTensor(0).observation = [1.0, 2.0, 0.0]
ObservationString(0) = "Location=O->A, movement=False, t=1, destination='D->aft_D'"
PublicObservationString() = "Location=O->A, movement=False, t=1, destination='D->aft_D'"
PrivateObservationString(0) = "Location=O->A, movement=False, t=1, destination='D->aft_D'"
ObservationTensor(0) = [1.0, 2.0, 0.0]
Rewards() = [0.0]
Returns() = [0]
LegalActions() = [0]
StringLegalActions() = ["Vehicle 0 reach a sink node or its destination."]

# Apply action "Vehicle 0 reach a sink node or its destination."
action: 0

# State 5
# Arrived at O->A, with travel time 3.0, t=2_mean_field
IsTerminal() = True
History() = [0, 2, 0, 0]
HistoryString() = "0, 2, 0, 0"
IsChanceNode() = False
IsSimultaneousNode() = False
CurrentPlayer() = PlayerId.TERMINAL
InformationStateString(0) = "Arrived at O->A, with travel time 3.0, t=2_mean_field"
InformationStateTensor(0).observation = [1.0, 2.0, 2.0]
ObservationString(0) = "Arrived at O->A, with travel time 3.0, t=2_mean_field"
PublicObservationString() = "Arrived at O->A, with travel time 3.0, t=2_mean_field"
PrivateObservationString(0) = "Arrived at O->A, with travel time 3.0, t=2_mean_field"
ObservationTensor(0) = [1.0, 2.0, 2.0]
Rewards() = [-3.0]
Returns() = [-3.0]
